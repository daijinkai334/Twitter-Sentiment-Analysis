{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'clean_data.csv'\n",
    "file = pandas.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = file.Tweet\n",
    "y = file.Label\n",
    "seed = 334\n",
    "# 按（0.98, 0.01, 0.01）的比例划分训练、验证、测试集\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x, y, test_size=0.02, random_state=seed)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=0.5, random_state=seed)\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Set | <center><font size=3>Size | <center><font size=3>Proportion |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>Train | <center><font size=3>1565182 | <center><font size=3>98.00% |\n",
    "| <center><font size=3>Validation | <center><font size=3>15971 | <center><font size=3>1.00% |\n",
    "| <center><font size=3>Test | <center><font size=3>15982 | <center><font size=3>1.00% |\n",
    "| <center><font size=3>Total | <center><font size=3>1597125 | <center><font size=3>100.00% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入预生成的词向量长度为100的word2vec模型\n",
    "model_cbow = KeyedVectors.load('./word2vec_models/model_cbow_100.word2vec')\n",
    "model_sg = KeyedVectors.load('./word2vec_models/model_sg_100.word2vec')\n",
    "\n",
    "# len(model_cbow.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Vocabulary Size | <center><font size=3>Vector Size| <center><font size=3>Vector Type |\n",
    "|--|--|--|\n",
    "| <center><font size=3>268681 | <center><font size=3>100 | <center><font size=3>numpy.ndarray |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建不同词频阈值的分词器，并将每个句子中的单词序列变为数字序列\n",
    "num_words=100000\n",
    "\n",
    "tokenizer_100000 = Tokenizer(num_words=num_words)\n",
    "# tokenizer_150000 = Tokenizer(num_words=150000)\n",
    "# tokenizer_200000 = Tokenizer(num_words=200000)\n",
    "# tokenizer_250000 = Tokenizer(num_words=250000)\n",
    "\n",
    "tokenizer_100000.fit_on_texts(x_train)\n",
    "# tokenizer_150000.fit_on_texts(x_train)\n",
    "# tokenizer_200000.fit_on_texts(x_train)\n",
    "# tokenizer_250000.fit_on_texts(x_train)\n",
    "\n",
    "sequences_train_100000 = tokenizer_100000.texts_to_sequences(x_train)\n",
    "# sequences_train_150000 = tokenizer_150000.texts_to_sequences(x_train)\n",
    "# sequences_train_200000 = tokenizer_200000.texts_to_sequences(x_train)\n",
    "# sequences_200000 = tokenizer_250000.texts_to_sequences(x_train)\n",
    "\n",
    "# 利用训练集上的分词器对验证集和测试集进行序列化\n",
    "sequences_validation_100000 = tokenizer_100000.texts_to_sequences(x_validation)\n",
    "sequences_test_100000 = tokenizer_100000.texts_to_sequences(x_test)\n",
    "# sequences_validation_150000 = tokenizer_150000.texts_to_sequences(x_validation)\n",
    "# sequences_test_150000 = tokenizer_150000.texts_to_sequences(x_test)\n",
    "# sequences_validation_200000 = tokenizer_200000.texts_to_sequences(x_validation)\n",
    "# sequences_test_200000 = tokenizer_200000.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 确定句子中最大的单词数\n",
    "len_max = 0\n",
    "for x in x_train:\n",
    "    temp = len(x.split())\n",
    "    if temp > len_max:\n",
    "        len_max = temp\n",
    "\n",
    "# len_max\n",
    "\n",
    "# 这里不能用sequence的长度来确定句子的最大长度，因为原始数据中存在乱码，如下面的两个print语句所示，而预处理的时候没有考虑到这一点\n",
    "# print(sequences_100000[510758:510759])\n",
    "# print(x_train[510758:510759])\n",
    "# x_train 输入num时，为index=num的元素，输入num1:num2时，为第num1行到第num2行的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对于变长的句子进行padding使其成为维度相同的标准输入\n",
    "x_train_pad = pad_sequences(sequences_train_100000, maxlen=70, padding='post')\n",
    "x_validation_pad = pad_sequences(sequences_validation_100000, maxlen=70, padding='post')\n",
    "x_test_pad = pad_sequences(sequences_test_100000, maxlen=70, padding='post')\n",
    "\n",
    "# x_train_pad[510758:510759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建嵌入矩阵（即在输入神经网络时将标号转为向量的索引）\n",
    "embedding_matrix_cbow = np.zeros((num_words, 100))\n",
    "embedding_matrix_sg = np.zeros((num_words, 100))\n",
    "embedding_matrix_cbow_sg = np.zeros((num_words, 200))\n",
    "for word, rank in tokenizer_100000.word_index.items():\n",
    "    if rank >= num_words:\n",
    "        break\n",
    "    if word in model_cbow:\n",
    "        embedding_matrix_cbow[rank] = model_cbow[word]\n",
    "    if word in model_sg:\n",
    "        embedding_matrix_sg[rank] = model_sg[word]\n",
    "    if (word in model_cbow) and (word in model_sg):\n",
    "        embedding_matrix_cbow_sg[rank] = np.append(model_cbow[word],model_sg[word])\n",
    "\n",
    "# 某些词如‘quot’等不具有实意的词在word2vec模型中没有对应的词向量，但词频极高，现在的处理会将其视为零向量\n",
    "\n",
    "# print(x_train[1:2])\n",
    "# sequences_train_100000[1:2]\n",
    "# print(embedding_matrix_cbow[297])\n",
    "# print(model_cbow['play'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)\n",
    "        \n",
    "# model.fit(params, ..., callbacks[TB(log_dir='...')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 618s 395us/step - loss: 0.4219 - acc: 0.8050 - val_loss: 0.4035 - val_acc: 0.8187\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 614s 392us/step - loss: 0.3999 - acc: 0.8174 - val_loss: 0.3953 - val_acc: 0.8222\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 619s 396us/step - loss: 0.3927 - acc: 0.8215 - val_loss: 0.3949 - val_acc: 0.8211\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 543s 347us/step - loss: 0.3885 - acc: 0.8239 - val_loss: 0.3983 - val_acc: 0.8185\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 537s 343us/step - loss: 0.3858 - acc: 0.8255 - val_loss: 0.3935 - val_acc: 0.8250\n"
     ]
    }
   ],
   "source": [
    "model_cnn_embedding_matrix_cbow = Sequential()\n",
    "model_cnn_embedding_matrix_cbow.add(Embedding(100000, 100, weights=[embedding_matrix_cbow], input_length=70, trainable=False))\n",
    "model_cnn_embedding_matrix_cbow.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_embedding_matrix_cbow.add(GlobalMaxPooling1D())\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(256, activation='relu'))\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(128, activation='relu'))\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_embedding_matrix_cbow.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_embedding_matrix_cbow.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_embedding_matrix_cbow/')])\n",
    "model_cnn_embedding_matrix_cbow.save('./models/model_cnn_embedding_matrix_cbow.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 524s 335us/step - loss: 0.4133 - acc: 0.8100 - val_loss: 0.3961 - val_acc: 0.8192\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 523s 334us/step - loss: 0.3915 - acc: 0.8221 - val_loss: 0.3930 - val_acc: 0.8239\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 519s 332us/step - loss: 0.3844 - acc: 0.8260 - val_loss: 0.3891 - val_acc: 0.8249\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 523s 334us/step - loss: 0.3799 - acc: 0.8286 - val_loss: 0.3887 - val_acc: 0.8260\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 537s 343us/step - loss: 0.3769 - acc: 0.8302 - val_loss: 0.3918 - val_acc: 0.8259\n"
     ]
    }
   ],
   "source": [
    "model_cnn_embedding_matrix_sg = Sequential()\n",
    "model_cnn_embedding_matrix_sg.add(Embedding(100000, 100, weights=[embedding_matrix_sg], input_length=70, trainable=False))\n",
    "model_cnn_embedding_matrix_sg.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_embedding_matrix_sg.add(GlobalMaxPooling1D())\n",
    "model_cnn_embedding_matrix_sg.add(Dense(256, activation='relu'))\n",
    "model_cnn_embedding_matrix_sg.add(Dense(128, activation='relu'))\n",
    "model_cnn_embedding_matrix_sg.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_embedding_matrix_sg.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_embedding_matrix_sg.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_embedding_matrix_sg/')])\n",
    "model_cnn_embedding_matrix_sg.save('./models/model_cnn_embedding_matrix_sg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Embedding Matrix | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|\n",
    "| <center><font size=3>CBOW | <center><font size=3>0.3858 | <center><font size=3>0.8255 | <center><font size=3>0.3935 | <center><font size=3>0.8250 | <center><font size=3>2931 |\n",
    "| <center><font size=3>SG | <center><font size=3>0.3769 | <center><font size=3>0.8302 | <center><font size=3>0.3918 | <center><font size=3>0.8259 | <center><font size=3>2626 |\n",
    "| <center><font size=3>CBOW+SG | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    " <center> **Filter Num = 100, Filter Size = 2, Strides = 1, Partition = 1, Epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 697s 445us/step - loss: 0.4158 - acc: 0.8087 - val_loss: 0.4027 - val_acc: 0.8212\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 690s 441us/step - loss: 0.3932 - acc: 0.8213 - val_loss: 0.3904 - val_acc: 0.8232\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 677s 433us/step - loss: 0.3858 - acc: 0.8255 - val_loss: 0.3893 - val_acc: 0.8266\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 680s 434us/step - loss: 0.3817 - acc: 0.8280 - val_loss: 0.3951 - val_acc: 0.8269\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 683s 436us/step - loss: 0.3787 - acc: 0.8297 - val_loss: 0.3891 - val_acc: 0.8266\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_num_100 = Sequential()\n",
    "model_cnn_filter_num_100.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_100.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_100.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_100.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_100.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_100.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_100.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_100.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_filter_num_100/')])\n",
    "model_cnn_filter_num_100.save('./models/model_cnn_filter_num_100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 463s 296us/step - loss: 0.4210 - acc: 0.8055 - val_loss: 0.4033 - val_acc: 0.8154\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 476s 304us/step - loss: 0.4013 - acc: 0.8169 - val_loss: 0.4010 - val_acc: 0.8162\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 469s 299us/step - loss: 0.3956 - acc: 0.8198 - val_loss: 0.3959 - val_acc: 0.8205\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 472s 301us/step - loss: 0.3925 - acc: 0.8217 - val_loss: 0.3967 - val_acc: 0.8224\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 470s 301us/step - loss: 0.3905 - acc: 0.8230 - val_loss: 0.3949 - val_acc: 0.8201\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_num_50 = Sequential()\n",
    "model_cnn_filter_num_50.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_50.add(Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_50.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_50.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_50.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_50.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_50.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_50.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_filter_num_50/')])\n",
    "model_cnn_filter_num_50.save('./models/model_cnn_filter_num_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 920s 588us/step - loss: 0.4132 - acc: 0.8101 - val_loss: 0.3953 - val_acc: 0.8209\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 930s 594us/step - loss: 0.3891 - acc: 0.8237 - val_loss: 0.3955 - val_acc: 0.8230\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 933s 596us/step - loss: 0.3808 - acc: 0.8284 - val_loss: 0.3865 - val_acc: 0.8286\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 948s 606us/step - loss: 0.3761 - acc: 0.8307 - val_loss: 0.3813 - val_acc: 0.8296\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 949s 606us/step - loss: 0.3725 - acc: 0.8329 - val_loss: 0.3828 - val_acc: 0.8261\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_num_150 = Sequential()\n",
    "model_cnn_filter_num_150.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_150.add(Conv1D(filters=150, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_150.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_150.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_150.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_150.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_150.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_150.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_filter_num_150/')])\n",
    "model_cnn_filter_num_150.save('./models/model_cnn_filter_num_150.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Filter Num | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>50 | <center><font size=3>0.3905 | <center><font size=3>0.8230 | <center><font size=3>0.3949 | <center><font size=3>0.8201 | <center><font size=3>2350 |\n",
    "| <center><font size=3>100 | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>150 | <center><font size=3>0.3725 | <center><font size=3>0.8329 | <center><font size=3>0.3828 | <center><font size=3>0.8261 | <center><font size=3>4680 |\n",
    " <center> **Filter Size = 2, Strides = 1, Partition = 1, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 812s 519us/step - loss: 0.4142 - acc: 0.8097 - val_loss: 0.3955 - val_acc: 0.8176\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 808s 516us/step - loss: 0.3897 - acc: 0.8238 - val_loss: 0.3860 - val_acc: 0.8261\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 817s 522us/step - loss: 0.3817 - acc: 0.8282 - val_loss: 0.3818 - val_acc: 0.8283\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 831s 531us/step - loss: 0.3767 - acc: 0.8308 - val_loss: 0.3798 - val_acc: 0.8309\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 837s 535us/step - loss: 0.3730 - acc: 0.8329 - val_loss: 0.3827 - val_acc: 0.8269\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_size_3 = Sequential()\n",
    "model_cnn_filter_size_3.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_size_3.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_size_3.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_size_3.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_size_3.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_size_3.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_size_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_size_3.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_filter_size_3')])\n",
    "model_cnn_filter_size_3.save('./models/model_cnn_filter_size_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 1075s 687us/step - loss: 0.4162 - acc: 0.8088 - val_loss: 0.3974 - val_acc: 0.8154\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 1052s 672us/step - loss: 0.3893 - acc: 0.8237 - val_loss: 0.3940 - val_acc: 0.8261\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 1048s 669us/step - loss: 0.3796 - acc: 0.8295 - val_loss: 0.3864 - val_acc: 0.8240\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 1059s 677us/step - loss: 0.3733 - acc: 0.8326 - val_loss: 0.3884 - val_acc: 0.8252\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 1069s 683us/step - loss: 0.3689 - acc: 0.8350 - val_loss: 0.3914 - val_acc: 0.8257\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_size_5 = Sequential()\n",
    "model_cnn_filter_size_5.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_size_5.add(Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_size_5.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_size_5.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_size_5.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_size_5.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_size_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_size_5.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_filter_size_5')])\n",
    "model_cnn_filter_size_5.save('./models/model_cnn_filter_size_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Filter Size | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>2 | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>3 | <center><font size=3>0.3730 | <center><font size=3>0.8329 | <center><font size=3>0.3827 | <center><font size=3>0.8269 | <center><font size=3>4105 |\n",
    "| <center><font size=3>5 | <center><font size=3>0.3689 | <center><font size=3>0.8350 | <center><font size=3>0.3914 | <center><font size=3>0.8257 | <center><font size=3>5303 |\n",
    " <center> **Filter Num = 100, Strides = 1, Partition = 1, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 528s 338us/step - loss: 0.4305 - acc: 0.7995 - val_loss: 0.4142 - val_acc: 0.8097\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 587s 375us/step - loss: 0.4098 - acc: 0.8116 - val_loss: 0.4210 - val_acc: 0.8104\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 587s 375us/step - loss: 0.4029 - acc: 0.8153 - val_loss: 0.4098 - val_acc: 0.8138\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 606s 387us/step - loss: 0.3984 - acc: 0.8181 - val_loss: 0.4073 - val_acc: 0.8161\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 551s 352us/step - loss: 0.3958 - acc: 0.8194 - val_loss: 0.4037 - val_acc: 0.8183\n"
     ]
    }
   ],
   "source": [
    "model_cnn_stride_2 = Sequential()\n",
    "model_cnn_stride_2.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_stride_2.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=2))\n",
    "model_cnn_stride_2.add(GlobalMaxPooling1D())\n",
    "model_cnn_stride_2.add(Dense(256, activation='relu'))\n",
    "model_cnn_stride_2.add(Dense(128, activation='relu'))\n",
    "model_cnn_stride_2.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_stride_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_stride_2.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_stride_2/')])\n",
    "model_cnn_stride_2.save('./models/model_cnn_stride_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 3691s 2ms/step - loss: 0.4820 - acc: 0.7663 - val_loss: 0.4627 - val_acc: 0.7756\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 349s 223us/step - loss: 0.4634 - acc: 0.7782 - val_loss: 0.4587 - val_acc: 0.7817\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 365s 233us/step - loss: 0.4571 - acc: 0.7821 - val_loss: 0.4565 - val_acc: 0.7821\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 374s 239us/step - loss: 0.4535 - acc: 0.7845 - val_loss: 0.4546 - val_acc: 0.7834\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 390s 249us/step - loss: 0.4506 - acc: 0.7862 - val_loss: 0.4565 - val_acc: 0.7819\n"
     ]
    }
   ],
   "source": [
    "model_cnn_stride_3 = Sequential()\n",
    "model_cnn_stride_3.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_stride_3.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=3))\n",
    "model_cnn_stride_3.add(GlobalMaxPooling1D())\n",
    "model_cnn_stride_3.add(Dense(256, activation='relu'))\n",
    "model_cnn_stride_3.add(Dense(128, activation='relu'))\n",
    "model_cnn_stride_3.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_stride_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_stride_3.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_stride_3/')])\n",
    "model_cnn_stride_3.save('./models/model_cnn_stride_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Stride | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>1 | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>2 | <center><font size=3>0.3958 | <center><font size=3>0.8194 | <center><font size=3>0.4037 | <center><font size=3>0.8183 | <center><font size=3>2859 |\n",
    "| <center><font size=3>3 | <center><font size=3>0.4506 | <center><font size=3>0.7862 | <center><font size=3>0.4565 | <center><font size=3>0.7819 | <center><font size=3>1823 |\n",
    " <center> **Filter Size = 2, Filter Num = 100, Partition = 1, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 515s 329us/step - loss: 0.4154 - acc: 0.8085 - val_loss: 0.3961 - val_acc: 0.8198\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 612s 391us/step - loss: 0.3928 - acc: 0.8215 - val_loss: 0.3888 - val_acc: 0.8223\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 642s 410us/step - loss: 0.3858 - acc: 0.8256 - val_loss: 0.3843 - val_acc: 0.8278\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 584s 373us/step - loss: 0.3814 - acc: 0.8281 - val_loss: 0.3834 - val_acc: 0.8258\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 583s 373us/step - loss: 0.3785 - acc: 0.8296 - val_loss: 0.3840 - val_acc: 0.8271\n"
     ]
    }
   ],
   "source": [
    "model_cnn_2_partition_pooling = Sequential()\n",
    "model_cnn_2_partition_pooling.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_2_partition_pooling.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_2_partition_pooling.add(MaxPooling1D(pool_size=35))\n",
    "model_cnn_2_partition_pooling.add(Flatten())\n",
    "model_cnn_2_partition_pooling.add(Dense(256, activation='relu'))\n",
    "model_cnn_2_partition_pooling.add(Dense(128, activation='relu'))\n",
    "model_cnn_2_partition_pooling.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_2_partition_pooling.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_2_partition_pooling.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_2_partition_pooling')])\n",
    "model_cnn_2_partition_pooling.save('./models/model_cnn_2_partition_pooling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 618s 395us/step - loss: 0.4155 - acc: 0.8088 - val_loss: 0.3955 - val_acc: 0.8211\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 582s 372us/step - loss: 0.3929 - acc: 0.8216 - val_loss: 0.3906 - val_acc: 0.8212\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 634s 405us/step - loss: 0.3855 - acc: 0.8256 - val_loss: 0.3847 - val_acc: 0.8244\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 621s 397us/step - loss: 0.3811 - acc: 0.8285 - val_loss: 0.3855 - val_acc: 0.8295\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 489s 313us/step - loss: 0.3778 - acc: 0.8300 - val_loss: 0.3818 - val_acc: 0.8279\n"
     ]
    }
   ],
   "source": [
    "model_cnn_3_partition_pooling = Sequential()\n",
    "model_cnn_3_partition_pooling.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_3_partition_pooling.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_3_partition_pooling.add(MaxPooling1D(pool_size=24))\n",
    "model_cnn_3_partition_pooling.add(Flatten())\n",
    "model_cnn_3_partition_pooling.add(Dense(256, activation='relu'))\n",
    "model_cnn_3_partition_pooling.add(Dense(128, activation='relu'))\n",
    "model_cnn_3_partition_pooling.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_3_partition_pooling.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_3_partition_pooling.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_3_partition_pooling')])\n",
    "model_cnn_3_partition_pooling.save('./models/model_cnn_3_partition_pooling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 487s 311us/step - loss: 0.4145 - acc: 0.8095 - val_loss: 0.3935 - val_acc: 0.8216\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 519s 331us/step - loss: 0.3924 - acc: 0.8220 - val_loss: 0.3884 - val_acc: 0.8242\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 508s 325us/step - loss: 0.3852 - acc: 0.8262 - val_loss: 0.3851 - val_acc: 0.8266\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 508s 325us/step - loss: 0.3812 - acc: 0.8281 - val_loss: 0.3804 - val_acc: 0.8294\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 496s 317us/step - loss: 0.3782 - acc: 0.8299 - val_loss: 0.3825 - val_acc: 0.8291\n"
     ]
    }
   ],
   "source": [
    "model_cnn_4_partition_pooling = Sequential()\n",
    "model_cnn_4_partition_pooling.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_4_partition_pooling.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_4_partition_pooling.add(MaxPooling1D(pool_size=18))\n",
    "model_cnn_4_partition_pooling.add(Flatten())\n",
    "model_cnn_4_partition_pooling.add(Dense(256, activation='relu'))\n",
    "model_cnn_4_partition_pooling.add(Dense(128, activation='relu'))\n",
    "model_cnn_4_partition_pooling.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_4_partition_pooling.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_4_partition_pooling.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_4_partition_pooling')])\n",
    "model_cnn_4_partition_pooling.save('./models/model_cnn_4_partition_pooling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 512s 327us/step - loss: 0.4143 - acc: 0.8097 - val_loss: 0.3954 - val_acc: 0.8215\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 494s 316us/step - loss: 0.3915 - acc: 0.8227 - val_loss: 0.3885 - val_acc: 0.8243\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 499s 319us/step - loss: 0.3837 - acc: 0.8270 - val_loss: 0.3874 - val_acc: 0.8237\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 510s 326us/step - loss: 0.3791 - acc: 0.8292 - val_loss: 0.3838 - val_acc: 0.8278\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 518s 331us/step - loss: 0.3762 - acc: 0.8312 - val_loss: 0.3880 - val_acc: 0.8252\n"
     ]
    }
   ],
   "source": [
    "model_cnn_5_partition_pooling = Sequential()\n",
    "model_cnn_5_partition_pooling.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_5_partition_pooling.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_5_partition_pooling.add(MaxPooling1D(pool_size=14))\n",
    "model_cnn_5_partition_pooling.add(Flatten())\n",
    "model_cnn_5_partition_pooling.add(Dense(256, activation='relu'))\n",
    "model_cnn_5_partition_pooling.add(Dense(128, activation='relu'))\n",
    "model_cnn_5_partition_pooling.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_5_partition_pooling.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_5_partition_pooling.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_5_partition_pooling')])\n",
    "model_cnn_5_partition_pooling.save('./models/model_cnn_5_partition_pooling.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Partition | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>1 | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>2 | <center><font size=3>0.3758 | <center><font size=3>0.8296 | <center><font size=3>0.3840 | <center><font size=3>0.8271 | <center><font size=3>2936 |\n",
    "| <center><font size=3>3 | <center><font size=3>0.3778 | <center><font size=3>0.8300 | <center><font size=3>0.3818 | <center><font size=3>0.8279 | <center><font size=3>2944 |\n",
    "| <center><font size=3>4 | <center><font size=3>0.3782 | <center><font size=3>0.8299 | <center><font size=3>0.3825 | <center><font size=3>0.8291 | <center><font size=3>2516 |\n",
    "| <center><font size=3>5 | <center><font size=3>0.3762 | <center><font size=3>0.8312 | <center><font size=3>0.3880 | <center><font size=3>0.8252 | <center><font size=3>2533 |\n",
    " <center> **Filter Size = 2, Filter Num = 100, Stride = 1, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_max(x):\n",
    "    x.top_k\n",
    "    \n",
    "def k_max_output_shape(input_shape, k):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3\n",
    "    shape[-1] = k\n",
    "    return tuple(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_2_max_pooling = Sequential()\n",
    "model_cnn_2_max_pooling.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_2_max_pooling.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_2_max_pooling.add(Lambda(k_max, output_shape=k_max_output_shape(2)))\n",
    "model_cnn_2_max_pooling.add(Flatten())\n",
    "model_cnn_2_max_pooling.add(Dense(256, activation='relu'))\n",
    "model_cnn_2_max_pooling.add(Dense(128, activation='relu'))\n",
    "model_cnn_2_max_pooling.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_2_max_pooling.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_2_max_pooling.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_2_max_pooling')])\n",
    "model_cnn_2_max_pooling.save('./models/model_cnn_2_max_pooling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_trainable_true = Sequential()\n",
    "model_cnn_trainable_true.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=True))\n",
    "model_cnn_trainable_true.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_trainable_true.add(GlobalMaxPooling1D())\n",
    "model_cnn_trainable_true.add(Dense(256, activation='relu'))\n",
    "model_cnn_trainable_true.add(Dense(128, activation='relu'))\n",
    "model_cnn_trainable_true.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_trainable_true.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_trainable_true.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_trainable_true/')])\n",
    "model_cnn_trainable_true.save('./models/model_cnn_trainable_true.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Trainable | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>False | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>True | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3> |\n",
    " <center> **Filter Size = 2, Filter Num = 100, Strides = 1, Partition = 1, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 70, 200)           20000000  \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 70, 200, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 69, 199, 70)       350       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 34, 99, 70)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 33, 98, 50)        14050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 16, 49, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 15, 48, 30)        6030      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 7, 24, 30)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5040)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               1290496   \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 21,343,951\n",
      "Trainable params: 21,343,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "    256/1565182 [..............................] - ETA: 17:41:49 - loss: 0.7038 - acc: 0.5039"
     ]
    }
   ],
   "source": [
    "model_cnn_conv2d = Sequential()\n",
    "model_cnn_conv2d.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_conv2d.add(Reshape((-1, 200, 1)))\n",
    "model_cnn_conv2d.add(Conv2D(filters=70, kernel_size=(2, 2), padding='valid', activation='relu', strides=(1, 1)))\n",
    "model_cnn_conv2d.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "model_cnn_conv2d.add(Conv2D(filters=50, kernel_size=(2, 2), padding='valid', activation='relu', strides=(1, 1)))\n",
    "model_cnn_conv2d.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "model_cnn_conv2d.add(Conv2D(filters=30, kernel_size=(2, 2), padding='valid', activation='relu', strides=(1, 1)))\n",
    "model_cnn_conv2d.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "model_cnn_conv2d.add(Flatten())\n",
    "model_cnn_conv2d.add(Dense(256, activation='relu'))\n",
    "model_cnn_conv2d.add(Dense(128, activation='relu'))\n",
    "model_cnn_conv2d.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_conv2d.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_conv2d.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_conv2d/')])\n",
    "model_cnn_conv2d.save('./models/model_cnn_conv2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Network | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>Conv1D | <center><font size=3>0.3787 | <center><font size=3>0.8297 | <center><font size=3>0.3891 | <center><font size=3>0.8266 | <center><font size=3>3427 |\n",
    "| <center><font size=3>Conv2D | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3>0. | <center><font size=3> |\n",
    " <center> **Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/10\n",
      " 345056/1565182 [=====>........................] - ETA: 23:14 - loss: 0.4370 - acc: 0.7967"
     ]
    }
   ],
   "source": [
    "input_type = Input(shape=(70,), dtype='float64')\n",
    "tweet_encoder = Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False)(input_type)\n",
    "model_2 = Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "model_2 = GlobalMaxPooling1D()(model_2)\n",
    "model_3 = Conv1D(filters=50, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "model_3 = GlobalMaxPooling1D()(model_3)\n",
    "model_5 = Conv1D(filters=50, kernel_size=5, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "model_5 = GlobalMaxPooling1D()(model_5)\n",
    "merged = concatenate([model_2, model_3, model_5], axis=1)\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "model_cnn_mixed = Model(inputs=[input_type], outputs=[output])\n",
    "model_cnn_mixed.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_mixed.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=10, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/model_cnn_mixed/')])\n",
    "model_cnn_mixed.save('./models/model_cnn_mixed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mix.evaluate(x=x_test_pad, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "FPR, TPR, threshold = roc_curve(y_test, model_mix.predict(x_test_pad))\n",
    "roc_auc = auc(FPR, TPR)\n",
    "plt.plot(FPR, TPR)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
