{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'clean_data.csv'\n",
    "file = pandas.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = file.Tweet\n",
    "y = file.Label\n",
    "seed = 334\n",
    "# 按（0.98, 0.01, 0.01）的比例划分训练、验证、测试集\n",
    "x_train, x_validation_test, y_train, y_validation_test = train_test_split(x, y, test_size=0.02, random_state=seed)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_test, y_validation_test, test_size=0.5, random_state=seed)\n",
    "\n",
    "# print(len(x))\n",
    "# print(len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Set | <center><font size=3>Size | <center><font size=3>Proportion |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>Train | <center><font size=3>1565182 | <center><font size=3>98.00% |\n",
    "| <center><font size=3>Validation | <center><font size=3>15971 | <center><font size=3>1.00% |\n",
    "| <center><font size=3>Test | <center><font size=3>15982 | <center><font size=3>1.00% |\n",
    "| <center><font size=3>Total | <center><font size=3>1597125 | <center><font size=3>100.00% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入预生成的词向量长度为100的word2vec模型\n",
    "model_cbow = KeyedVectors.load('model_cbow_100.word2vec')\n",
    "model_sg = KeyedVectors.load('model_sg_100.word2vec')\n",
    "\n",
    "# len(model_cbow.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Vocabulary Size | <center><font size=3>Vector Size| <center><font size=3>Vector Type |\n",
    "|--|--|--|\n",
    "| <center><font size=3>268681 | <center><font size=3>100 | <center><font size=3>numpy.ndarray |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建不同词频阈值的分词器，并将每个句子中的单词序列变为数字序列\n",
    "num_words = 100000\n",
    "tokenizer_100000 = Tokenizer(num_words=num_words)\n",
    "# tokenizer_150000 = Tokenizer(num_words=150000)\n",
    "# tokenizer_200000 = Tokenizer(num_words=200000)\n",
    "# tokenizer_250000 = Tokenizer(num_words=250000)\n",
    "\n",
    "tokenizer_100000.fit_on_texts(x_train)\n",
    "# tokenizer_150000.fit_on_texts(x_train)\n",
    "# tokenizer_200000.fit_on_texts(x_train)\n",
    "# tokenizer_250000.fit_on_texts(x_train)\n",
    "\n",
    "sequences_train_100000 = tokenizer_100000.texts_to_sequences(x_train)\n",
    "# sequences_150000 = tokenizer_150000.texts_to_sequences(x_train)\n",
    "# sequences_200000 = tokenizer_200000.texts_to_sequences(x_train)\n",
    "# sequences_200000 = tokenizer_250000.texts_to_sequences(x_train)\n",
    "\n",
    "# 利用训练集上的分词器对验证集进行序列化\n",
    "sequences_validation_100000 = tokenizer_100000.texts_to_sequences(x_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 确定句子中最大的单词数\n",
    "len_max = 0\n",
    "for x in x_train:\n",
    "    temp = len(x.split())\n",
    "    if temp > len_max:\n",
    "        len_max = temp\n",
    "\n",
    "# len_max\n",
    "\n",
    "# 这里不能用sequence的长度来确定句子的最大长度，因为原始数据中存在乱码，如下面的两个print语句所示，而预处理的时候没有考虑到这一点\n",
    "# print(sequences_100000[510758:510759])\n",
    "# print(x_train[510758:510759])\n",
    "# x_train 输入num时，为index=num的元素，输入num1:num2时，为第num1行到第num2行的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对于变长的句子进行padding使其成为维度相同的标准输入\n",
    "x_train_pad = pad_sequences(sequences_train_100000, maxlen=70, padding='post')\n",
    "x_validation_pad = pad_sequences(sequences_validation_100000, maxlen=70, padding='post')\n",
    "\n",
    "# x_train_pad[510758:510759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建嵌入矩阵（即在输入神经网络时将标号转为向量的索引）\n",
    "embedding_matrix_cbow = np.zeros((num_words, 100))\n",
    "embedding_matrix_sg = np.zeros((num_words, 100))\n",
    "embedding_matrix_cbow_sg = np.zeros((num_words, 200))\n",
    "for word, rank in tokenizer_100000.word_index.items():\n",
    "    if rank >= num_words:\n",
    "        break\n",
    "    if word in model_cbow:\n",
    "        embedding_matrix_cbow[rank] = model_cbow[word]\n",
    "    if word in model_sg:\n",
    "        embedding_matrix_sg[rank] = model_sg[word]\n",
    "    if (word in model_cbow) and (word in model_sg):\n",
    "        embedding_matrix_cbow_sg[rank] = np.append(model_cbow[word],model_sg[word])\n",
    "\n",
    "# 某些词如‘quot’等不具有实意的词在word2vec模型中没有对应的词向量，但词频极高，现在的处理会将其视为零向量\n",
    "\n",
    "# print(x_train[1:2])\n",
    "# sequences_train_100000[1:2]\n",
    "# print(embedding_matrix_cbow[297])\n",
    "# print(model_cbow['play'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 611s 390us/step - loss: 0.4225 - acc: 0.8044 - val_loss: 0.4049 - val_acc: 0.8175\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 719s 459us/step - loss: 0.4004 - acc: 0.8172 - val_loss: 0.3966 - val_acc: 0.8190\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 704s 450us/step - loss: 0.3933 - acc: 0.8214 - val_loss: 0.3990 - val_acc: 0.8214\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 1108s 708us/step - loss: 0.3888 - acc: 0.8238 - val_loss: 0.3920 - val_acc: 0.8233\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 921s 589us/step - loss: 0.3858 - acc: 0.8257 - val_loss: 0.3906 - val_acc: 0.8250\n"
     ]
    }
   ],
   "source": [
    "model_cnn_embedding_matrix_cbow = Sequential()\n",
    "model_cnn_embedding_matrix_cbow.add(Embedding(100000, 100, weights=[embedding_matrix_cbow], input_length=70, trainable=False))\n",
    "model_cnn_embedding_matrix_cbow.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_embedding_matrix_cbow.add(GlobalMaxPooling1D())\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(256, activation='relu'))\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(128, activation='relu'))\n",
    "model_cnn_embedding_matrix_cbow.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_embedding_matrix_cbow.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_embedding_matrix_cbow.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_embedding_matrix_cbow/')])\n",
    "model_cnn_embedding_matrix_cbow.save('./models/model_cnn_embedding_matrix_cbow.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 750s 479us/step - loss: 0.4126 - acc: 0.8103 - val_loss: 0.3959 - val_acc: 0.8198\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 656s 419us/step - loss: 0.3913 - acc: 0.8223 - val_loss: 0.3949 - val_acc: 0.8247\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 705s 451us/step - loss: 0.3843 - acc: 0.8266 - val_loss: 0.3858 - val_acc: 0.8261\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 643s 411us/step - loss: 0.3799 - acc: 0.8284 - val_loss: 0.3868 - val_acc: 0.8248\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 698s 446us/step - loss: 0.3767 - acc: 0.8302 - val_loss: 0.3861 - val_acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "model_cnn_embedding_matrix_sg = Sequential()\n",
    "model_cnn_embedding_matrix_sg.add(Embedding(100000, 100, weights=[embedding_matrix_sg], input_length=70, trainable=False))\n",
    "model_cnn_embedding_matrix_sg.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_embedding_matrix_sg.add(GlobalMaxPooling1D())\n",
    "model_cnn_embedding_matrix_sg.add(Dense(256, activation='relu'))\n",
    "model_cnn_embedding_matrix_sg.add(Dense(128, activation='relu'))\n",
    "model_cnn_embedding_matrix_sg.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_embedding_matrix_sg.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_embedding_matrix_sg.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_embedding_matrix_sg/')])\n",
    "model_cnn_embedding_matrix_sg.save('./models/model_cnn_embedding_matrix_sg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Embedding Matrix | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|\n",
    "| <center><font size=3>CBOW | <center><font size=3>0.3858 | <center><font size=3>0.8257 | <center><font size=3>0.3906 | <center><font size=3>0.8250 | <center><font size=3>3460 |\n",
    "| <center><font size=3>SG | <center><font size=3>0.3767 | <center><font size=3>0.8302 | <center><font size=3>0.3861 | <center><font size=3>0.8276 | <center><font size=3>3452 |\n",
    "| <center><font size=3>CBOW+SG | <center><font size=3>0.3784 | <center><font size=3>0.8298 | <center><font size=3>0.3866 | <center><font size=3>0.8249 | <center><font size=3>4755 |\n",
    " <center> **Filter Num = 100, Filter Size = 2, Epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 874s 558us/step - loss: 0.4153 - acc: 0.8089 - val_loss: 0.4177 - val_acc: 0.8123\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 998s 637us/step - loss: 0.3929 - acc: 0.8217 - val_loss: 0.3870 - val_acc: 0.8273\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 931s 595us/step - loss: 0.3854 - acc: 0.8261 - val_loss: 0.3900 - val_acc: 0.8261\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 985s 629us/step - loss: 0.3812 - acc: 0.8284 - val_loss: 0.3846 - val_acc: 0.8246\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 967s 618us/step - loss: 0.3784 - acc: 0.8298 - val_loss: 0.3866 - val_acc: 0.8249\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_num_100 = Sequential()\n",
    "model_cnn_filter_num_100.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_100.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_100.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_100.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_100.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_100.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_100.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_100.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_filter_num_100/')])\n",
    "model_cnn_filter_num_100.save('./models/model_cnn_filter_num_100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 518s 331us/step - loss: 0.4215 - acc: 0.8050 - val_loss: 0.4072 - val_acc: 0.8141\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 652s 417us/step - loss: 0.4015 - acc: 0.8168 - val_loss: 0.3988 - val_acc: 0.8183\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 674s 431us/step - loss: 0.3956 - acc: 0.8201 - val_loss: 0.3966 - val_acc: 0.8195\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 636s 407us/step - loss: 0.3922 - acc: 0.8219 - val_loss: 0.3981 - val_acc: 0.8206\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 716s 457us/step - loss: 0.3901 - acc: 0.8230 - val_loss: 0.3948 - val_acc: 0.8203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a91613ba8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_filter_num_50 = Sequential()\n",
    "model_cnn_filter_num_50.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_50.add(Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_50.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_50.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_50.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_50.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_50.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_50.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_filter_num_50/')])\n",
    "model_cnn_filter_num_50.save('./models/model_cnn_filter_num_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 1254s 801us/step - loss: 0.4125 - acc: 0.8105 - val_loss: 0.3935 - val_acc: 0.8218\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 1350s 863us/step - loss: 0.3893 - acc: 0.8237 - val_loss: 0.3888 - val_acc: 0.8247\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 1014s 648us/step - loss: 0.3810 - acc: 0.8281 - val_loss: 0.3860 - val_acc: 0.8305\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 1239s 792us/step - loss: 0.3760 - acc: 0.8308 - val_loss: 0.3799 - val_acc: 0.8293\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 1204s 769us/step - loss: 0.3726 - acc: 0.8329 - val_loss: 0.3830 - val_acc: 0.8293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a93e3a208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_filter_num_150 = Sequential()\n",
    "model_cnn_filter_num_150.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_num_150.add(Conv1D(filters=150, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_num_150.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_num_150.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_num_150.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_num_150.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_num_150.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_num_150.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_filter_num_150/')])\n",
    "model_cnn_filter_num_150.save('./models/model_cnn_filter_num_150.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Filter Num | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>50 | <center><font size=3>0.3901 | <center><font size=3>0.8230 | <center><font size=3>0.3948 | <center><font size=3>0.8203 | <center><font size=3>3196 |\n",
    "| <center><font size=3>100 | <center><font size=3>0.3784 | <center><font size=3>0.8298 | <center><font size=3>0.3866 | <center><font size=3>0.8249 | <center><font size=3>4755 |\n",
    "| <center><font size=3>150 | <center><font size=3>0.3726 | <center><font size=3>0.8329 | <center><font size=3>0.3830 | <center><font size=3>0.8293 | <center><font size=3>6061 |\n",
    " <center> **Filter Size = 2, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 1013s 647us/step - loss: 0.4137 - acc: 0.8098 - val_loss: 0.3953 - val_acc: 0.8192\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 1150s 734us/step - loss: 0.3892 - acc: 0.8237 - val_loss: 0.3894 - val_acc: 0.8238\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 866s 553us/step - loss: 0.3811 - acc: 0.8282 - val_loss: 0.3844 - val_acc: 0.8239\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 869s 555us/step - loss: 0.3760 - acc: 0.8312 - val_loss: 0.3839 - val_acc: 0.8292\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 883s 564us/step - loss: 0.3729 - acc: 0.8331 - val_loss: 0.3817 - val_acc: 0.8267\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_size_3 = Sequential()\n",
    "model_cnn_filter_size_3.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_size_3.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_size_3.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_size_3.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_size_3.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_size_3.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_size_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_size_3.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_filter_size_3')])\n",
    "model_cnn_filter_size_3.save('./models/model_cnn_filter_size_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/5\n",
      "1565182/1565182 [==============================] - 1507s 963us/step - loss: 0.4165 - acc: 0.8086 - val_loss: 0.4049 - val_acc: 0.8172\n",
      "Epoch 2/5\n",
      "1565182/1565182 [==============================] - 1347s 861us/step - loss: 0.3899 - acc: 0.8235 - val_loss: 0.3875 - val_acc: 0.8264\n",
      "Epoch 3/5\n",
      "1565182/1565182 [==============================] - 1370s 875us/step - loss: 0.3798 - acc: 0.8293 - val_loss: 0.3835 - val_acc: 0.8289\n",
      "Epoch 4/5\n",
      "1565182/1565182 [==============================] - 1232s 787us/step - loss: 0.3737 - acc: 0.8323 - val_loss: 0.3852 - val_acc: 0.8277\n",
      "Epoch 5/5\n",
      "1565182/1565182 [==============================] - 1485s 949us/step - loss: 0.3692 - acc: 0.8348 - val_loss: 0.3926 - val_acc: 0.8272\n"
     ]
    }
   ],
   "source": [
    "model_cnn_filter_size_5 = Sequential()\n",
    "model_cnn_filter_size_5.add(Embedding(100000, 200, weights=[embedding_matrix_cbow_sg], input_length=70, trainable=False))\n",
    "model_cnn_filter_size_5.add(Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_filter_size_5.add(GlobalMaxPooling1D())\n",
    "model_cnn_filter_size_5.add(Dense(256, activation='relu'))\n",
    "model_cnn_filter_size_5.add(Dense(128, activation='relu'))\n",
    "model_cnn_filter_size_5.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_filter_size_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_cnn_filter_size_5.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=5, batch_size=32, verbose=1, callbacks=[TensorBoard(log_dir='./tmp/log/model_cnn_filter_size_5')])\n",
    "model_cnn_filter_size_5.save('./models/model_cnn_filter_size_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <center><font size=3>Filter Size | <center><font size=3>Loss | <center><font size=3>Acc | <center><font size=3>Val Loss | <center><font size=3>Val Acc | <center><font size=3>Time(s) |\n",
    "|--|--|--|--|--|--|\n",
    "| <center><font size=3>2 | <center><font size=3>0.3784 | <center><font size=3>0.8298 | <center><font size=3>0.3866 | <center><font size=3>0.8249 | <center><font size=3>4755 |\n",
    "| <center><font size=3>3 | <center><font size=3>0.3729 | <center><font size=3>0.8331 | <center><font size=3>0.3817 | <center><font size=3>0.8267 | <center><font size=3>4781 |\n",
    "| <center><font size=3>5 | <center><font size=3>0.3692 | <center><font size=3>0.8348 | <center><font size=3>0.3926 | <center><font size=3>0.8272 | <center><font size=3>6941 |\n",
    " <center> **Filter Num = 100, Epochs = 5, Embedding Matrix = CBOW+SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TB(TensorBoard):\n",
    "    def __init__(self, log_every=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.log_every = log_every\n",
    "        self.counter = 0\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.counter+=1\n",
    "        if self.counter%self.log_every==0:\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, self.counter)\n",
    "            self.writer.flush()\n",
    "        \n",
    "        super().on_batch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(params, ..., callbacks[TB(log_dir='...')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1565182 samples, validate on 15971 samples\n",
      "Epoch 1/1\n",
      "1565182/1565182 [==============================] - 352s 225us/step - loss: 0.4443 - acc: 0.7927 - val_loss: 0.4338 - val_acc: 0.8013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a94820a90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(100000, 100, weights=[embedding_matrix_cbow], input_length=70, trainable=False))\n",
    "model.add(Conv1D(filters=20, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train_pad, y_train, validation_data=(x_validation_pad, y_validation), epochs=1, batch_size=32, verbose=1, callbacks=[TB(log_dir='./tmp/log/try')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
